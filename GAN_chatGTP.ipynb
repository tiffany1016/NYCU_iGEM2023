{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ITPAfwvtOtz6KWQn6d-sAQzWll2hnuY3",
      "authorship_tag": "ABX9TyPi7zAp/eb0mY4K2INCbe0m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiffany1016/NYCU_iGEM2023/blob/web_crawler/2023_5_19_GAN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rt7_-vy-Cb5m",
        "outputId": "383e72f5-176a-4f9d-e627-da99a38cad7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May 23 10:35:36 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8    13W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv1D, Flatten\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "Teol-9CFCj3A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the generator network\n",
        "def make_generator_model(sequences):\n",
        "    max_seq_length = sequences.shape[1]\n",
        "    num_unique_chars = sequences.shape[2]\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(layers.Dense(256, input_dim=NOISE_DIM))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Hidden layers\n",
        "    model.add(layers.Dense(512))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    model.add(layers.Dense(1024))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(max_seq_length * num_unique_chars, activation='sigmoid'))\n",
        "    model.add(layers.Reshape((max_seq_length, num_unique_chars)))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the discriminator network\n",
        "def make_discriminator_model(sequences):\n",
        "    max_seq_length = sequences.shape[1]\n",
        "    num_unique_chars = sequences.shape[2]\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(layers.Flatten(input_shape=(max_seq_length, num_unique_chars)))\n",
        "\n",
        "    # Hidden layers\n",
        "    model.add(layers.Dense(512))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.LeakyReLU(alpha=0.2))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "4eh4faOgD4oc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss functions for the generator and discriminator\n",
        "def generator_loss(fake_output):\n",
        "    # Compute generator loss\n",
        "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output)\n",
        "    return loss\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    # Compute discriminator loss\n",
        "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
        "    loss = real_loss + fake_loss\n",
        "    return loss"
      ],
      "metadata": {
        "id": "MiYXOXubEB47"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer for the generator and discriminator\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "xxFJywk_EpHA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop\n",
        "def train_step(real_sequences):\n",
        "    # Generate random noise as input to the generator\n",
        "    noise = tf.random.normal([real_sequences.shape[0], NOISE_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        # Generate a sequence from the generator using the random noise\n",
        "        generated_sequences = generator(noise, training=True)\n",
        "\n",
        "        # Evaluate the discriminator on real and generated sequences\n",
        "        real_output = discriminator(real_sequences, training=True)\n",
        "        fake_output = discriminator(generated_sequences, training=True)\n",
        "\n",
        "        # Compute the generator and discriminator losses\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # Compute the gradients and update the generator and discriminator weights\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n"
      ],
      "metadata": {
        "id": "e9dKiWYPErvj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the main training function\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        for batch in dataset:\n",
        "            real_sequences = batch\n",
        "            train_step(real_sequences)\n",
        "\n",
        "        print('Epoch {} complete'.format(epoch + 1))\n"
      ],
      "metadata": {
        "id": "BL39C7nrEtcT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRm6Ve5ZEB2R",
        "outputId": "95a30107-857b-497f-d661-68394957f788"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = np.loadtxt(\"/content/drive/MyDrive/Colab Notebooks/iGEM2023_model/random_protein_input.txt\", dtype=np.str_)  # Update \"your_dataset.txt\" with your actual dataset file name\n",
        "\n",
        "encoding_mapping={}\n",
        "# Convert the dataset to one-hot encoding\n",
        "def one_hot_encode(sequences):\n",
        "    unique_characters = list(set(\"\".join(sequences)))\n",
        "    print(unique_characters)\n",
        "    char_to_idx = {char: idx for idx, char in enumerate(unique_characters)}\n",
        "\n",
        "    num_sequences = len(sequences)\n",
        "    max_seq_length = max(len(seq) for seq in sequences)\n",
        "    num_unique_chars = len(unique_characters)\n",
        "\n",
        "    encoded_sequences = np.zeros((num_sequences, max_seq_length, num_unique_chars), dtype=np.float32)\n",
        "\n",
        "    for i, seq in enumerate(sequences):\n",
        "        for j, char in enumerate(seq):\n",
        "            encoded_sequences[i, j, char_to_idx[char]] = 1.0\n",
        "\n",
        "    for char in unique_characters:\n",
        "      encode_list=np.zeros(len(unique_characters))\n",
        "      encode_list[char_to_idx[char]]=1\n",
        "      encoding_mapping[char]=encode_list\n",
        "    \n",
        "    \n",
        "    print(encoding_mapping)\n",
        "\n",
        "    return encoded_sequences\n",
        "\n",
        "\n",
        "# One-hot encode the dataset\n",
        "dataset = one_hot_encode(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoQAskVnEwWD",
        "outputId": "6cec80b5-8103-4d21-ec2e-3aede59fe1ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['L', 'W', 'P', 'Q', 'R', 'I', 'C', 'K', 'V', 'H', 'Y', 'N', 'E', 'F', 'S', 'M', 'T', 'G', 'D', 'A']\n",
            "{'L': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'W': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'P': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'Q': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'R': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'I': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'C': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'K': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'V': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'H': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'Y': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'N': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'E': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'F': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "       0., 0., 0.]), 'S': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "       0., 0., 0.]), 'M': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "       0., 0., 0.]), 'T': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "       0., 0., 0.]), 'G': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       1., 0., 0.]), 'D': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 1., 0.]), 'A': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "       0., 0., 1.])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# num_sequences, max_seq_length, num_unique_chars\n",
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yuVgbCzS_oH",
        "outputId": "55943d4c-013d-4392-8549-1724b0afe3ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 93, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dataset:\n",
        "  print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWgmlLs2U-Am",
        "outputId": "ecef9bef-c293-4fe2-824a-c58493e4bb15"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " ...\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "[[0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 1. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the number of training epochs\n",
        "num_epochs = 10  # Update with the desired number of epochs\n",
        "\n",
        "# Set other hyperparameters\n",
        "BATCH_SIZE = 64  # Update with your desired batch size\n",
        "NOISE_DIM = 100  # Update with the dimensionality of your noise input\n",
        "\n",
        "# Create instances of the generator and discriminator\n",
        "generator = make_generator_model(dataset)\n",
        "discriminator = make_discriminator_model(dataset)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(dataset).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "iCiidGVxE7Pn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the training\n",
        "train(dataset, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7DVhCl5FKE6",
        "outputId": "f35a0523-1319-4ddc-f445-b95e986329eb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fa07ccc1120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fa07ccc1120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete\n",
            "Epoch 2 complete\n",
            "Epoch 3 complete\n",
            "Epoch 4 complete\n",
            "Epoch 5 complete\n",
            "Epoch 6 complete\n",
            "Epoch 7 complete\n",
            "Epoch 8 complete\n",
            "Epoch 9 complete\n",
            "Epoch 10 complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# decode_sequences\n",
        "def decode_sequences(encoded_sequences, encoding_mapping):\n",
        "    decoded_sequences = []\n",
        "    for encoded_sequence in encoded_sequences:\n",
        "        decoded_sequence = ''\n",
        "        for encoded_character in encoded_sequence:\n",
        "            for character, binary_vector in encoding_mapping.items():\n",
        "                if np.array_equal(encoded_character, binary_vector):\n",
        "                    decoded_sequence += character\n",
        "                    # print(\"ckp\")\n",
        "                    # print(character)\n",
        "                    break\n",
        "        decoded_sequences.append(decoded_sequence)\n",
        "    return decoded_sequences\n",
        "\n",
        "# Example usage\n",
        "# encoded_sequences = [[0, 1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0]]\n",
        "# encoding_mapping = {'A': [0, 1, 0, 0], 'C': [0, 0, 1, 0], 'G': [1, 0, 0, 0], 'T': [0, 0, 0, 1]}\n",
        "\n",
        "# decoded_sequences = decode_sequences(encoded_sequences, encoding_mapping)\n",
        "# print(decoded_sequences)"
      ],
      "metadata": {
        "id": "eMoFGSg8FODO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate new DNA sequences using the trained generator\n",
        "def generate_proteins(num_samples):\n",
        "    # Generate random noise as input to the generator\n",
        "    noise = tf.random.normal([num_samples, NOISE_DIM])\n",
        "\n",
        "    # Generate sequences using the generator\n",
        "    generated_sequences = generator(noise, training=False)\n",
        "    print(generated_sequences)\n",
        "\n",
        "    modify_seq = np.array(generated_sequences.numpy())\n",
        "\n",
        "    for seq in modify_seq:\n",
        "      for char in seq:\n",
        "        max_index=0\n",
        "        for i in range(0, generated_sequences.shape[2]):\n",
        "          if char[i]>char[max_index]: \n",
        "            max_index=i\n",
        "        for i in range(0, generated_sequences.shape[2]):\n",
        "          if i==max_index:\n",
        "            char[i]=1\n",
        "          else:\n",
        "            char[i]=0\n",
        "\n",
        "\n",
        "    print(modify_seq)\n",
        "\n",
        "    # Decode the generated sequences from one-hot encoding to protein sequences\n",
        "    decoded_sequences = decode_sequences(modify_seq, encoding_mapping)  # Use the correct decoding function and provide the encoding_mapping\n",
        "\n",
        "    return decoded_sequences"
      ],
      "metadata": {
        "id": "SZpXZaTfFTSo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate 10 new protein sequences\n",
        "generated_proteins = generate_proteins(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L53nsphNFW5k",
        "outputId": "e0d0b2bd-cb61-440c-ba49-295c73603987"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[0.5166759  0.46842536 0.5366596  ... 0.50497913 0.47429183 0.510549  ]\n",
            "  [0.49059826 0.49786198 0.46004006 ... 0.5030415  0.51095456 0.54538804]\n",
            "  [0.49469882 0.48425227 0.49530977 ... 0.5677939  0.48007748 0.4298949 ]\n",
            "  ...\n",
            "  [0.52343476 0.50395167 0.5049937  ... 0.5172256  0.5210657  0.39149347]\n",
            "  [0.53176767 0.5180647  0.53126085 ... 0.55271393 0.51085544 0.58967966]\n",
            "  [0.43254054 0.55665636 0.5370724  ... 0.4652047  0.46956617 0.5807766 ]]\n",
            "\n",
            " [[0.47497588 0.507018   0.5010536  ... 0.53040445 0.47548428 0.47224352]\n",
            "  [0.48599228 0.5160231  0.53956276 ... 0.4817179  0.54571575 0.5263266 ]\n",
            "  [0.50018024 0.52093613 0.51878625 ... 0.5182923  0.5168199  0.4938498 ]\n",
            "  ...\n",
            "  [0.512412   0.4790662  0.5009404  ... 0.52493614 0.42207953 0.52922446]\n",
            "  [0.47363263 0.49989244 0.5428209  ... 0.51431555 0.43763772 0.56967205]\n",
            "  [0.52182555 0.49965346 0.45969626 ... 0.4841601  0.48732218 0.5084871 ]]\n",
            "\n",
            " [[0.45484185 0.50359994 0.5402182  ... 0.5075404  0.45111936 0.5620997 ]\n",
            "  [0.48955166 0.495204   0.4973781  ... 0.4779842  0.42741886 0.48498785]\n",
            "  [0.46338508 0.51821315 0.44669116 ... 0.534456   0.51810116 0.4796256 ]\n",
            "  ...\n",
            "  [0.44900757 0.4555271  0.48123923 ... 0.45844147 0.525977   0.5448242 ]\n",
            "  [0.46810865 0.52497804 0.45021188 ... 0.48946136 0.47948307 0.45416632]\n",
            "  [0.514613   0.48714525 0.5549183  ... 0.48696584 0.42855665 0.5564801 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.4829489  0.46796638 0.5017745  ... 0.55282444 0.49679434 0.5186445 ]\n",
            "  [0.5102866  0.52266324 0.49251997 ... 0.47010273 0.5398917  0.47624522]\n",
            "  [0.41109765 0.5412339  0.5335682  ... 0.47305816 0.55821973 0.45488343]\n",
            "  ...\n",
            "  [0.44676077 0.5463982  0.52655166 ... 0.48366007 0.4617329  0.44787768]\n",
            "  [0.46604362 0.4034181  0.49058    ... 0.47454354 0.57556736 0.64145577]\n",
            "  [0.49239936 0.50966495 0.47601292 ... 0.51375943 0.43279484 0.59689176]]\n",
            "\n",
            " [[0.45421007 0.50781524 0.43020657 ... 0.57183033 0.46877348 0.6194458 ]\n",
            "  [0.54227954 0.47163582 0.48400125 ... 0.46281347 0.5329339  0.54322654]\n",
            "  [0.4583411  0.47768918 0.46980244 ... 0.46947497 0.5760573  0.4957574 ]\n",
            "  ...\n",
            "  [0.3891386  0.4645064  0.5360465  ... 0.50481147 0.508605   0.5518837 ]\n",
            "  [0.47834766 0.5001085  0.44691446 ... 0.43559924 0.495446   0.53262496]\n",
            "  [0.47828043 0.48853993 0.5387928  ... 0.45486745 0.5001641  0.58397067]]\n",
            "\n",
            " [[0.5303567  0.5665678  0.46740013 ... 0.50904787 0.5087247  0.53156966]\n",
            "  [0.45486996 0.52357125 0.50860554 ... 0.4796401  0.5322908  0.5555307 ]\n",
            "  [0.4792395  0.4909228  0.5010174  ... 0.5114558  0.5095356  0.42903855]\n",
            "  ...\n",
            "  [0.47457433 0.54833144 0.5145325  ... 0.49125853 0.46674395 0.5408264 ]\n",
            "  [0.46530244 0.48922688 0.44905442 ... 0.48533174 0.506234   0.5361038 ]\n",
            "  [0.46857095 0.5230288  0.5377552  ... 0.5261136  0.5519318  0.5083665 ]]], shape=(10, 93, 20), dtype=float32)\n",
            "[[[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 1. 0.]\n",
            "  ...\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 1.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  ...\n",
            "  [0. 1. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the generated protein sequences\n",
        "for protein in generated_proteins:\n",
        "    print(protein)"
      ],
      "metadata": {
        "id": "IimOxOZHYXRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbd1d09-fc53-420d-98f6-dfb8bbb05f2c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QTCFPIVGMCKRHELPFPDKKICQKVGYRDFCAKIEWTNNGNFAYHGRDAESLRVDFGGQILRSNIMNISPDDDLEKQCAKQEGLRIQATVVA\n",
            "ESTELWKKMTKVKRLDSMENHKRRFWVTWIMLFVIMVYEGSQANGGRRAFEVNVLLTVCPMLHKHKAVPHELRAYNFIVPPSLWGDVWQIFAK\n",
            "YKNWTADWLTYYGQAPAGDYKNEQSMFFVYFYMFIHHMAFCTPKWDQDHVKGTWHDAGDPDGHSHRHCTMNHQMYHAISMSLEPHKVQIPAYT\n",
            "QDWFTARMVPKFHKPPFYDKNNRQFKGYQIFRASIPDMNKIFDVGCGCCTESYGLDPEPWFLKHQFAVPRPFHMDPLQCNPLTRGLSPQIKVG\n",
            "GTHGAQHVYYGFLDEHLARNLMAQSKSVNFTAEWVWRVNKSIPFLCCWVNNRIQLLYGPQTGRDHLWVPILMWRLSRIWFNEHRYPVFPIQKL\n",
            "QAKVCAWVYRKDDDLDWCRIHNVATNPYRIGEFGIETWNKRIAGGHWRRSNRHKLFPEVFVHMHGFAKQMWPSCYSRVCSKTDWMTVSQIKMP\n",
            "GADCWIWAYNGVCVLPIMTNWLDQFGFYIFGCLTIWWHRNVCAVGLWAELPQRKLDPDCFIFKFMLNNQMGVWCYLLKKDSQEGRHAAMIIHK\n",
            "QINCWICIWMKADLLDYMSLKNLRGIHYDTFTADIEWETARWPAHTRSDAYGTRFWTGPQGLHSHRQSKNVRYRLHFEVYKLNIPDWVAIQVA\n",
            "AQDCFICGWDKKTQPHNVRYKNRNRMQMWTFCAWIFGMADMCNGKKADRFNSHNYCTSPVIQHLDRHKKHDHWNYQKQWYKHNLITLRIIQYE\n",
            "WAHCPPCIYVKTDYPRRVTQPMRRGADHIFHTLVRPREFMRQLGQATRAESTVPVGEHPETLHNTIAAKLDRIKDQFIKDKMMQKTWFQAWVE\n"
          ]
        }
      ]
    }
  ]
}
